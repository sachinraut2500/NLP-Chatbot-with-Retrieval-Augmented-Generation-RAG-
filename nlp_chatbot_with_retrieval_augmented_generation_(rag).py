# -*- coding: utf-8 -*-
"""NLP Chatbot with Retrieval-Augmented Generation (RAG).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pKYgrwW1BbCxeOcU-7lO2w31ebT7-_yO
"""

# rag_chatbot.py

from sentence_transformers import SentenceTransformer, util
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Sample knowledge base
documents = [
    "The Eiffel Tower is located in Paris, France.",
    "Python is a popular programming language for data science.",
    "The Earth revolves around the Sun once every 365 days.",
    "OpenAI developed the ChatGPT language model.",
    "Tesla is known for its electric vehicles."
]

def embed_documents(docs, model):
    return model.encode(docs, convert_to_tensor=True)

def retrieve_context(question, doc_embeddings, docs, model, top_k=1):
    q_embedding = model.encode(question, convert_to_tensor=True)
    hits = util.semantic_search(q_embedding, doc_embeddings, top_k=top_k)[0]
    retrieved_text = "\n".join([docs[hit['corpus_id']] for hit in hits])
    return retrieved_text

def generate_response(context, question, tokenizer, gpt_model):
    prompt = f"Context: {context}\n\nQuestion: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = gpt_model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

def main():
    print("Loading models...")
    embedder = SentenceTransformer("all-MiniLM-L6-v2")
    gpt_model = AutoModelForCausalLM.from_pretrained("gpt2")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    print("Embedding documents...")
    doc_embeddings = embed_documents(documents, embedder)

    print("\n--- RAG Chatbot ---")
    while True:
        question = input("You: ")
        if question.lower() in ['exit', 'quit']:
            break

        context = retrieve_context(question, doc_embeddings, documents, embedder)
        response = generate_response(context, question, tokenizer, gpt_model)
        print(response.strip().split("Answer:")[-1])
        print("-" * 40)

if __name__ == "__main__":
    main()